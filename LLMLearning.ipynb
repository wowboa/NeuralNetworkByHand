{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8124002e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jerry\\anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\Jerry\\anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 508.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 11.16 GiB is allocated by PyTorch, and 1.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8032\\1353404916.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"THUDM/codegeex2-6b-int4\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"THUDM/codegeex2-6b-int4\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m             return model_class.from_pretrained(\n\u001b[0m\u001b[0;32m    512\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2875\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_contexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2876\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2877\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2878\u001b[0m         \u001b[1;31m# Check first if we are `from_pt`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\THUDM\\codegeex2-6b-int4\\1b019c64b138e407d158c01e47bf28f9affb8b4c\\modeling_chatglm.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, empty_init, device)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_sequence_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChatGLMModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mempty_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mempty_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    857\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\THUDM\\codegeex2-6b-int4\\1b019c64b138e407d158c01e47bf28f9affb8b4c\\modeling_chatglm.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, device, empty_init)\u001b[0m\n\u001b[0;32m    755\u001b[0m                                               dtype=config.torch_dtype)\n\u001b[0;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGLMTransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m         self.output_layer = init_method(nn.Linear, config.hidden_size, config.padded_vocab_size, bias=False,\n\u001b[0m\u001b[0;32m    758\u001b[0m                                         dtype=config.torch_dtype, **init_kwargs)\n\u001b[0;32m    759\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_seq_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_seq_len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\utils\\init.py\u001b[0m in \u001b[0;36mskip_init\u001b[1;34m(module_cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mfinal_device\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'device'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'device'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'meta'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodule_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_empty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfinal_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto_empty\u001b[1;34m(self, device, recurse)\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \"\"\"\n\u001b[1;32m-> 1032\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    823\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \"\"\"\n\u001b[1;32m-> 1032\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_prims_common\\wrappers.py\u001b[0m in \u001b[0;36m_fn\u001b[1;34m(out, *args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m                         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_attr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m             assert (\n\u001b[0;32m    252\u001b[0m                 \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorLike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_refs\\__init__.py\u001b[0m in \u001b[0;36mempty_like\u001b[1;34m(a, dtype, device, layout, pin_memory, requires_grad, memory_format)\u001b[0m\n\u001b[0;32m   4753\u001b[0m     )\n\u001b[0;32m   4754\u001b[0m     \u001b[1;31m# identity perm is [2, 1, 0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4755\u001b[1;33m     return torch.empty_permuted(\n\u001b[0m\u001b[0;32m   4756\u001b[0m         \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4757\u001b[0m         \u001b[0mlogical_to_physical_perm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 508.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 11.16 GiB is allocated by PyTorch, and 1.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/codegeex2-6b-int4\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/codegeex2-6b-int4\", trust_remote_code=True, device='cuda')\n",
    "model = model.eval()\n",
    "\n",
    "# remember adding a language tag for better performance\n",
    "prompt = \"# language: Python\\n# write a bubble sort function\\n\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs, max_length=256, top_k=1)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d97c75b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f220794a1824f0ea6f3d026bdebae74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/245 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jerry\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jerry\\.cache\\huggingface\\hub\\models--THUDM--codegeex2-6b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a81ad2539c45ab9e990fbe6f4da5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenization_chatglm.py:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/codegeex2-6b:\n",
      "- tokenization_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68886ac915d14481921559d624a6fcd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/codegeex2-6b\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "945d95a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a211040fec4309bec84298514e54de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"THUDM/codegeex2-6b\", trust_remote_code=True).quantize(8).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2d19d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "\n",
    "# remember adding a language tag for better performance\n",
    "prompt = \"# language: Python\\n# write a bubble sort function\\n\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e21d6699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "C:\\Users\\Jerry/.cache\\huggingface\\modules\\transformers_modules\\THUDM\\codegeex2-6b\\3cb3f8fa305c8188c6c997d0be2ccc4b87ba6f7f\\modeling_chatglm.py:228: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(query_layer, key_layer, value_layer,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# language: Python\n",
      "# write a bubble sort function\n",
      "\n",
      "\n",
      "def bubble_sort(list):\n",
      "    for i in range(len(list) - 1):\n",
      "        for j in range(len(list) - 1 - i):\n",
      "            if list[j] > list[j + 1]:\n",
      "                list[j], list[j + 1] = list[j + 1], list[j]\n",
      "    return list\n",
      "\n",
      "\n",
      "print(bubble_sort([5, 2, 1, 8, 4]))\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs, max_length=256, top_k=1)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3217c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 661/661 [00:00<00:00, 1.63kB/s]\n",
      "Downloading: 100%|██████████| 48.0/48.0 [00:00<00:00, 165B/s]\n",
      "Downloading: 100%|██████████| 138/138 [00:00<00:00, 362B/s]\n",
      "Downloading: 100%|██████████| 11.1k/11.1k [00:00<00:00, 27.8kB/s]\n",
      "Downloading: 100%|██████████| 1.59M/1.59M [00:00<00:00, 2.76MB/s]\n",
      "Downloading: 100%|██████████| 942M/942M [02:25<00:00, 6.79MB/s] \n",
      "Downloading: 100%|██████████| 4.71k/4.71k [00:00<00:00, 17.0kB/s]\n",
      "Downloading: 100%|██████████| 6.70M/6.70M [00:02<00:00, 3.02MB/s]\n",
      "Downloading: 100%|██████████| 1.26k/1.26k [00:00<00:00, 4.05kB/s]\n",
      "Downloading: 100%|██████████| 2.65M/2.65M [00:01<00:00, 2.52MB/s]\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Downloading model checkpoint to a local dir model_dir\n",
    "# model_dir = snapshot_download('qwen/Qwen-1_8B',cache_dir='./models/Qwen-1_8B')\n",
    "model_dir = snapshot_download('qwen/Qwen2-0.5B',cache_dir='./models/Qwen2-0_5B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5da592ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./models/Qwen2-0_5B/qwen/Qwen2-0___5B',trust_remote_code=True) # trust_remote_code=True 必须要加\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "     \"./models/Qwen2-0_5B/qwen/Qwen2-0___5B\",\n",
    "     device_map=\"cuda:0\",\n",
    "     trust_remote_code=True # 默认为True，表示将下载来着Hugging Face或其他在线资源的配置文件。若False则需指定本地配置文件。\n",
    " ).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "727a0189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfd369428614d7d88d40c454a0d7f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "# Model names: \"Qwen/Qwen-1_8B\", \"Qwen/Qwen-1_8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('./models/Qwen-1_8B',trust_remote_code=True) # trust_remote_code=True 必须要加\n",
    "# use bf16\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n",
    "# use fp16\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n",
    "# use cpu only\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B\", device_map=\"cpu\", trust_remote_code=True).eval()\n",
    "# use auto mode, automatically select precision based on the device.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./models/Qwen-1_8B/qwen/Qwen-1_8B\",\n",
    "    device_map=\"cuda:0\",\n",
    "    trust_remote_code=True # 默认为True，表示将下载来着Hugging Face或其他在线资源的配置文件。若False则需指定本地配置文件。\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b15bd8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "c:\\Users\\Jerry\\anaconda3\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:580: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我最近感觉很累，没有精神，想睡觉，但是又睡不着，有什么办法可以缓解吗？\n",
      "您好，根据您的症状，可能是由于睡眠不足或缺乏运动引起的疲劳感。以下是一些缓解疲劳的方法：\n",
      "\n",
      "1. 坚持规律的睡眠时间，每晚7-8小时。\n",
      "\n",
      "2. 保持良好的睡眠环境，保持房间安静、凉爽、舒适。\n",
      "\n",
      "3. 避免在睡前过度使用电子设备，如手机、电脑等。\n",
      "\n",
      "4. 增加运动量，如散步、慢跑、瑜伽等。\n",
      "\n",
      "5. 饮食上要注意均衡，多摄入富含维生素和矿物质的食物，如水果、蔬菜、全谷类食品等。\n",
      "\n",
      "如果以上方法不能缓解您的症状，建议您咨询医生或专业医生的意见。\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"你好！\", return_tensors=\"pt\").to(\"cuda:0\")\n",
    "#outputs = model(**inputs)\n",
    "\n",
    "inputs = inputs.to(model.device)\n",
    "pred = model.generate(**inputs)\n",
    "print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a3af2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词典大小: 151851\n"
     ]
    }
   ],
   "source": [
    "print(\"词典大小:\",tokenizer\n",
    "      .vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a9885e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[108386,   6313]], device='cuda:0'), 'token_type_ids': tensor([[0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b733586",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ed31b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89f3f353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4f92a4493245e68b11370ced1c04e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen.tiktoken:   0%|          | 0.00/2.56M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jerry\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jerry\\.cache\\huggingface\\hub\\models--qwen--Qwen-1_8B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# trust_remote_code默认是True，表示意味着当您使用 from_pretrained() 方法加载模型配置文件时，它将下载来自 Hugging Face 模型中心或其他在线资源的配置文件。这是一个方便的默认行为，因为通常这些配置文件是由官方提供的，且是可信的。\n",
    "# 下载Qwen-1_8B的在线配置文件\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"qwen/Qwen-1_8B\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deef590e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/qwen-1_8b/tokenizer_config.json',\n",
       " './models/qwen-1_8b/special_tokens_map.json',\n",
       " './models/qwen-1_8b/qwen.tiktoken',\n",
       " './models/qwen-1_8b/added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存Qwen-1_8B配置文件\n",
    "# special_tokens_map.json：映射文件，里面包含 unknown token 等特殊字符的映射关系；\n",
    "# tokenizer_config.json：分词器配置文件，存储构建分词器需要的参数；\n",
    "# vocab.txt：词表，一行一个 token，行号就是对应的 token ID（从 0 开始）。\n",
    "tokenizer.save_pretrained(\"./models/qwen-1_8b/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fde9b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiktoken._educational import *\n",
    "enc = SimpleBytePairEncoding.from_tiktoken(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba2240c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;5;167m�\u001b[48;5;179m�\u001b[48;5;185m�\u001b[48;5;77m�\u001b[48;5;80m�\u001b[48;5;68m�\u001b[48;5;134m�\u001b[48;5;167m�\u001b[48;5;179m�\u001b[0m\n",
      "\u001b[48;5;167m�\u001b[48;5;179m�\u001b[48;5;185m�\u001b[48;5;77m�\u001b[48;5;80m�\u001b[48;5;68m�\u001b[48;5;134m�\u001b[48;5;167m�\u001b[0m\n",
      "\u001b[48;5;167m�\u001b[48;5;179m�\u001b[48;5;185m�\u001b[48;5;77m�\u001b[48;5;80m�\u001b[48;5;68m�\u001b[48;5;134m�\u001b[0m\n",
      "\u001b[48;5;167m�\u001b[48;5;179m�\u001b[48;5;185m是\u001b[48;5;77m�\u001b[48;5;80m�\u001b[48;5;68m�\u001b[0m\n",
      "\u001b[48;5;167m�\u001b[48;5;179m�\u001b[48;5;185m是\u001b[48;5;77m�\u001b[48;5;80m�\u001b[0m\n",
      "\u001b[48;5;167m你\u001b[48;5;179m是\u001b[48;5;185m�\u001b[48;5;77m�\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m�\u001b[48;5;179m�\u001b[48;5;185m�\u001b[0m\n",
      "\u001b[48;5;167m�\u001b[48;5;179m�\u001b[0m\n",
      "\u001b[48;5;167m？\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[57668, 21043, 39013, 223, 11571]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(\"你是谁？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c4e4ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'\\xe4\\xbd\\xa0\\xe6\\x98\\xaf', b'\\xe8\\xb0\\x81', b'\\xef\\xbc\\x9f']\n",
      "['你是', '谁', '？']\n"
     ]
    }
   ],
   "source": [
    "sequence = \"你是谁？\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)\n",
    "print(list(map(lambda x: x.decode(encoding=\"utf-8\"), tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a48ae6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105043, 100165, 11319]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids) #分词表里ID对应的是中文的Base64编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63c01a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [105043, 100165, 11319], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer(\"你是谁？\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a165b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
