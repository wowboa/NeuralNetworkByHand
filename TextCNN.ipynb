{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from  torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文本数据\n",
    "def read_data(train_or_test,num=None):\n",
    "    # train数据输入'train',test数据输入'test'\n",
    "    with open(os.path.join('./data',train_or_test+'.txt'),'r',encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    text_all_num = len(lines)\n",
    "    text = []\n",
    "    labels = []\n",
    "    if num and num <= text_all_num :\n",
    "        for i in range(num):\n",
    "            text.append(lines[i].split('\\t')[0])\n",
    "            labels.append(lines[i].split('\\t')[1].strip())\n",
    "        print('已完成{}条数据读取'.format(num))\n",
    "    else:\n",
    "        for line in lines:\n",
    "            text.append(line.split('\\t')[0])\n",
    "            labels.append(line.split('\\t')[1].strip())\n",
    "        print('已完成全部{}条数据读取'.format(text_all_num))\n",
    "    return text,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建词库和随机词向量\n",
    "def built_curpus(text,embedding_num):\n",
    "    word_index_dict = {\"<PAD>\":0,\"<UNK>\":1}\n",
    "    for sentense in text:\n",
    "        for word in sentense:\n",
    "            word_index_dict[word] = word_index_dict.get(word,len(word_index_dict))\n",
    "    return word_index_dict,nn.Embedding(len(word_index_dict),embedding_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data\\\\test.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m text,labels \u001b[38;5;241m=\u001b[39m \u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m word_index_dict,words_embedding \u001b[38;5;241m=\u001b[39m built_curpus(text,\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m, in \u001b[0;36mread_data\u001b[1;34m(train_or_test, num)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_data\u001b[39m(train_or_test,num\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# train数据输入'train',test数据输入'test'\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrain_or_test\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      5\u001b[0m         lines \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m      6\u001b[0m     text_all_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(lines)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data\\\\test.txt'"
     ]
    }
   ],
   "source": [
    "text,labels = read_data('test')\n",
    "word_index_dict,words_embedding = built_curpus(text,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self,text,labels,word_index_dict,seq_max_len):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.word_index_dict = word_index_dict\n",
    "        self.seq_max_len = seq_max_len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sentense = self.text[index][:self.seq_max_len]\n",
    "        label = int(self.labels[index])\n",
    "\n",
    "        sentense_idx = [self.word_index_dict.get(word,1) for word in sentense]\n",
    "        sentense_idx = sentense_idx + [0]*(self.seq_max_len - len(sentense_idx))\n",
    "        sentense_idx = torch.tensor(sentense_idx).unsqueeze(dim=0)\n",
    "        return sentense_idx,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 56,  57, 133, 134,  13,  14,  19,  96,  17,  18, 135, 136, 127,  13,\n",
       "          125, 126,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0]]),\n",
       " '3')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = TextDataset(text,labels,word_index_dict,100)\n",
    "test_data.__getitem__(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模块\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,kernel_s,embedding_num,seq_max_len,hidden_num):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Conv2d(in_channels=1,out_channels=hidden_num,kernel_size=(kernel_s,embedding_num))\n",
    "        self.act = nn.ReLU()\n",
    "        self.mxp = nn.MaxPool1d(kernel_size=(seq_max_len-kernel_s+1))\n",
    "    \n",
    "    def forward(self,batch_emb):\n",
    "        c = self.cnn.forward(batch_emb)\n",
    "        a = self.act.forward(c)\n",
    "        a = a.squeeze(dim=-1)\n",
    "        m = self.mxp.forward(a)\n",
    "        m = m.squeeze(dim=-1)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建TextCNN网络结构\n",
    "class TextCNNModel(nn.Module):\n",
    "    def __init__(self,embedding_matrix,seq_max_len,class_num,hidden_num):\n",
    "        super().__init__()\n",
    "        self.embedding_num = embedding_matrix.weight.shape[1]\n",
    "\n",
    "        self.block1 = Block(2,self.embedding_num,seq_max_len,hidden_num)\n",
    "        self.block2 = Block(3,self.embedding_num,seq_max_len,hidden_num)\n",
    "        self.block3 = Block(4,self.embedding_num,seq_max_len,hidden_num)\n",
    "        self.block4 = Block(5,self.embedding_num,seq_max_len,hidden_num)\n",
    "\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_num*4,class_num)\n",
    "        self.loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self,batch_idx,batch_label=None):\n",
    "        batch_emb = self.embedding_matrix(batch_idx)\n",
    "        b1_result = self.block1.forward(batch_emb)\n",
    "        b2_result = self.block1.forward(batch_emb)\n",
    "        b3_result = self.block1.forward(batch_emb)\n",
    "        b4_result = self.block1.forward(batch_emb)\n",
    "\n",
    "        feature = torch.cat([b1_result,b2_result,b3_result,b4_result],dim=1)\n",
    "        pre = self.classifier(feature)\n",
    "        \n",
    "        if batch_label is not None:\n",
    "            loss = self.loss_fun(pre,batch_label)\n",
    "            return loss\n",
    "        else:\n",
    "            return torch.argmax(pre,dim=-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已完成全部180000条数据读取\n",
      "已完成全部10000条数据读取\n",
      "loss:2.050\n",
      "acc = 23.66%\n",
      "loss:1.948\n",
      "acc = 28.85%\n",
      "loss:1.868\n",
      "acc = 32.45%\n",
      "loss:1.826\n",
      "acc = 34.39%\n",
      "loss:1.775\n",
      "acc = 34.98%\n",
      "loss:1.723\n",
      "acc = 35.53%\n",
      "loss:1.669\n",
      "acc = 38.02%\n",
      "loss:1.649\n",
      "acc = 39.56%\n",
      "loss:1.634\n",
      "acc = 41.20%\n",
      "loss:1.609\n",
      "acc = 42.37%\n"
     ]
    }
   ],
   "source": [
    "train_text,train_label = read_data(\"train\")\n",
    "dev_text,dev_label = read_data(\"dev\")\n",
    "\n",
    "embedding_num = 10\n",
    "seq_max_len = 20\n",
    "batch_size = 200\n",
    "epoch = 10\n",
    "lr = 0.001\n",
    "hidden_num = 2\n",
    "class_num = len(set(train_label))\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "word_2_index,words_embedding = built_curpus(train_text,embedding_num)\n",
    "\n",
    "train_dataset = TextDataset(train_text,train_label,word_2_index,seq_max_len)\n",
    "train_loader = DataLoader(train_dataset,batch_size,shuffle=False)\n",
    "\n",
    "dev_dataset = TextDataset(dev_text, dev_label, word_2_index, seq_max_len)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model = TextCNNModel(words_embedding,seq_max_len,class_num,hidden_num).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(),lr=lr)\n",
    "\n",
    "for e in range(epoch):\n",
    "    for batch_idx,batch_label in train_loader:\n",
    "        batch_idx = batch_idx.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "        loss = model.forward(batch_idx,batch_label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    print(f\"loss:{loss:.3f}\")\n",
    "\n",
    "    right_num = 0\n",
    "    for batch_idx,batch_label in dev_loader:\n",
    "        batch_idx = batch_idx.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "        pre = model.forward(batch_idx)\n",
    "        right_num += int(torch.sum(pre==batch_label))\n",
    "\n",
    "    print(f\"acc = {right_num/len(dev_text)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
